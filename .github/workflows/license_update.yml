name: Actualizar Licencias Taxi

on:
  schedule:
    - cron: '0 7 * * *'  # Ejecutar a las 07:00 UTC (08:00 u 09:00 hora EspaÃ±a segÃºn horario invierno/verano)
  workflow_dispatch:      # Permite ejecutar manualmente desde la pestaÃ±a "Actions"

jobs:
  update-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write     # Permiso CRÃTICO para poder guardar los archivos generados

    steps:
      - name: ğŸ“¥ Checkout del cÃ³digo
        uses: actions/checkout@v3

      - name: ğŸ Instalar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: ğŸ“¦ Instalar dependencias
        run: |
          pip install -r requirements.txt
        # AsegÃºrate de que requirements.txt estÃ© en la raÃ­z y tenga: pandas, numpy, requests, beautifulsoup4 (o lo que use tu scraper)

      - name: ğŸ•µï¸ Ejecutar Scraper
        run: python scripts/licencia_scrap.py
        # Esto ejecutarÃ¡ tu script de scraping.
        # IMPORTANTE: AsegÃºrate de que licencia_scrap.py guarde el JSON en la raÃ­z (o que procesado_licen.py sepa dÃ³nde buscarlo).

      - name: ğŸ“Š Ejecutar Procesado y EstadÃ­sticas
        run: python scripts/procesado_licen.py
        # Esto leerÃ¡ el archivo del paso anterior y generarÃ¡ los datos para la web (market_data/)

      - name: ğŸ’¾ Guardar cambios (Commit & Push)
        run: |
          git config --global user.name 'TaxiBot'
          git config --global user.email 'bot@noreply.github.com'
          
          # AÃ±adimos todo lo que haya cambiado (el JSON raw y los datos procesados)
          git add .
          
          # Hacemos commit solo si hay cambios para evitar errores
          git diff --quiet && git diff --staged --quiet || (git commit -m "ğŸ“ˆ ActualizaciÃ³n diaria: $(date +'%Y-%m-%d')" && git push)
